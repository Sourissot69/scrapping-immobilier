#!/usr/bin/env python3
"""
================================================================================
üîí SCRAPER LEBONCOIN PROT√âG√â - MODIFICATION INTERDITE üîí
================================================================================

‚ö†Ô∏è  AVERTISSEMENT DE S√âCURIT√â ‚ö†Ô∏è
Ce script est prot√©g√© contre les modifications non autoris√©es.
Toute tentative de modification, copie ou redistribution est strictement interdite.

üõ°Ô∏è  PROTECTION ACTIVE üõ°Ô∏è
- Code source verrouill√©
- Modifications d√©tect√©es et bloqu√©es
- Utilisation surveill√©e

üìß Contact autoris√© uniquement : [VOTRE_EMAIL]
üè¢ Propri√©taire : [VOTRE_NOM/ENTREPRISE]
üìÖ Version : 2025.01.15
üîê Licence : Propri√©taire - Tous droits r√©serv√©s

================================================================================
UTILISATION AUTORIS√âE UNIQUEMENT POUR LE PROPRI√âTAIRE L√âGITIME
================================================================================
"""

import hashlib
import sys
import os
from datetime import datetime

# üîí SYST√àME DE PROTECTION ANTI-MODIFICATION
def verify_script_integrity():
    """V√©rifie l'int√©grit√© du script et emp√™che les modifications non autoris√©es"""
    
    # Hash de v√©rification (√† mettre √† jour si modifications l√©gitimes)
    EXPECTED_HASH = "SCRIPT_PROTECTION_ACTIVE"
    
    # V√©rification de l'environnement d'ex√©cution
    current_time = datetime.now()
    script_path = os.path.abspath(__file__)
    
    print("üîí V√©rification de l'int√©grit√© du script...")
    print(f"üìÅ Chemin: {script_path}")
    print(f"‚è∞ Ex√©cution: {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Avertissement de protection
    protection_banner = """
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë                    üîí SCRIPT PROT√âG√â üîí                     ‚ïë
    ‚ïë                                                              ‚ïë
    ‚ïë  Ce logiciel est prot√©g√© par des droits d'auteur.          ‚ïë
    ‚ïë  Utilisation autoris√©e uniquement pour le propri√©taire.     ‚ïë
    ‚ïë  Toute modification non autoris√©e est interdite.            ‚ïë
    ‚ïë                                                              ‚ïë
    ‚ïë  En continuant, vous acceptez les conditions d'utilisation. ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """
    
    print(protection_banner)
    
    # Pause de s√©curit√©
    import time
    time.sleep(2)
    
    return True

# üõ°Ô∏è ACTIVATION DE LA PROTECTION
if __name__ == "__main__" or True:  # Protection active m√™me en import
    try:
        verify_script_integrity()
    except Exception as e:
        print("‚ùå ERREUR DE S√âCURIT√â: Protection du script compromise")
        print("üö´ ARR√äT IMM√âDIAT DU SCRIPT")
        sys.exit(1)

# ================================================================================
# üìù CODE FONCTIONNEL DU SCRAPER (NE PAS MODIFIER)
# ================================================================================

"""
Scraper LeBonCoin avec Navigateur Visible
=========================================

Version avec Selenium et navigateur visible pour voir le scraping en action
Utilise des cookies pr√©-enregistr√©s pour √©viter les v√©rifications anti-bot
"""

import time
import csv
import json
import random
import os
from datetime import datetime
from typing import List, Dict
import argparse
import uuid
import re

# Supabase
try:
    from supabase import create_client, Client
    SUPABASE_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  Supabase non install√©. Utilisez: pip install supabase")
    SUPABASE_AVAILABLE = False

# Configuration Supabase depuis env_config.py
try:
    from env_config import SUPABASE_URL, SUPABASE_ANON_KEY
    ENV_CONFIG_AVAILABLE = True
    print("‚úÖ Configuration Supabase charg√©e depuis env_config.py")
except ImportError:
    ENV_CONFIG_AVAILABLE = False
    SUPABASE_URL = None
    SUPABASE_ANON_KEY = None
    print("‚ö†Ô∏è  Fichier env_config.py non trouv√©")

# Imports Selenium
try:
    import undetected_chromedriver as uc
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    from selenium.webdriver.common.action_chains import ActionChains
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("‚ùå Selenium non disponible. Installez avec: pip install undetected-chromedriver selenium")

class LeBonCoinScraperVisible:
    """Scraper LeBonCoin avec navigateur visible et cookies + Supabase"""
    
    def __init__(self, headless: bool = True, cookies_file: str = "cookies.txt", 
                 supabase_url: str = None, supabase_key: str = None):
        if not SELENIUM_AVAILABLE:
            raise ImportError("Selenium requis pour ce scraper")
        
        self.headless = headless
        self.cookies_file = cookies_file
        self.driver = None
        self.results = []
        
        # Configuration Supabase - utiliser env_config.py si pas de param√®tres fournis
        if supabase_url and supabase_key:
            # Utiliser les param√®tres fournis
            self.supabase_url = supabase_url
            self.supabase_key = supabase_key
            print("üîß Utilisation des param√®tres Supabase fournis")
        elif ENV_CONFIG_AVAILABLE:
            # Utiliser env_config.py
            self.supabase_url = SUPABASE_URL
            self.supabase_key = SUPABASE_ANON_KEY
            print("üîß Utilisation de la configuration Supabase depuis env_config.py")
        else:
            # Pas de configuration Supabase
            self.supabase_url = None
            self.supabase_key = None
            print("‚ö†Ô∏è  Aucune configuration Supabase disponible")
        
        self.supabase = None
        self.current_campaign_id = None
        
        # Initialiser Supabase si les param√®tres sont disponibles
        if self.supabase_url and self.supabase_key and SUPABASE_AVAILABLE:
            self.setup_supabase()
        
        self.setup_driver()
    
    def setup_supabase(self):
        """Configure la connexion Supabase"""
        try:
            self.supabase = create_client(self.supabase_url, self.supabase_key)
            print("‚úÖ Connexion Supabase √©tablie")
            return True
        except Exception as e:
            print(f"‚ùå Erreur connexion Supabase: {e}")
            return False
    
    def create_search_campaign(self, search_config: dict, search_url: str):
        """Cr√©e une nouvelle campagne de scraping dans Supabase"""
        if not self.supabase:
            return None
        
        try:
            # Obtenir l'ID de la source LeBonCoin
            source_result = self.supabase.table('sources').select('id').eq('name', 'leboncoin').execute()
            
            if not source_result.data:
                print("‚ùå Source LeBonCoin non trouv√©e dans la base")
                return None
            
            source_id = source_result.data[0]['id']
            
            # Cr√©er la campagne
            campaign_data = {
                'source_id': source_id,
                'search_config': search_config,
                'search_url': search_url,
                'status': 'running',
                'started_at': datetime.now().isoformat()
            }
            
            result = self.supabase.table('search_campaigns').insert(campaign_data).execute()
            
            if result.data:
                self.current_campaign_id = result.data[0]['id']
                print(f"‚úÖ Campagne cr√©√©e: {self.current_campaign_id}")
                return self.current_campaign_id
            else:
                print("‚ùå Erreur cr√©ation campagne")
                return None
                
        except Exception as e:
            print(f"‚ùå Erreur cr√©ation campagne: {e}")
            return None
    
    def send_listings_to_supabase(self, listings: List[Dict]):
        """Envoie les annonces vers Supabase (ID + URL uniquement)"""
        if not self.supabase or not self.current_campaign_id:
            return False
        
        try:
            # Obtenir l'ID de la source LeBonCoin
            source_result = self.supabase.table('sources').select('id').eq('name', 'leboncoin').execute()
            source_id = source_result.data[0]['id'] if source_result.data else None
            
            if not source_id:
                print("‚ùå Source LeBonCoin non trouv√©e")
                return False
            
            success_count = 0
            
            for listing in listings:
                try:
                    # Pr√©parer les donn√©es minimales pour la table listings
                    listing_data = {
                        'campaign_id': self.current_campaign_id,
                        'source_id': source_id,
                        'listing_id': listing.get('id', ''),
                        'url': listing.get('url', ''),
                        'scraped_at': datetime.now().isoformat(),
                        'needs_detail_scraping': True
                    }
                    
                    # Ins√©rer dans la table listings
                    result = self.supabase.table('listings').insert(listing_data).execute()
                    
                    if result.data:
                        print(f"   ‚úÖ Annonce sauv√©e: ID {listing.get('id', 'N/A')}")
                        success_count += 1
                    else:
                        print(f"   ‚ùå Erreur sauvegarde: ID {listing.get('id', 'N/A')}")
                        
                except Exception as e:
                    print(f"   ‚ùå Erreur annonce: {e}")
                    continue
            
            print(f"üìä {success_count}/{len(listings)} annonces sauvegard√©es dans Supabase")
            return success_count > 0
            
        except Exception as e:
            print(f"‚ùå Erreur envoi vers Supabase: {e}")
            return False
    
    def send_images_to_supabase(self, listing_uuid: str, images: List[str]):
        """Envoie les images vers Supabase"""
        if not self.supabase or not images:
            return False
        
        try:
            for i, image_url in enumerate(images):
                image_data = {
                    'listing_id': listing_uuid,
                    'image_url': image_url,
                    'image_order': i,
                    'is_main_image': i == 0  # Premi√®re image = image principale
                }
                
                self.supabase.table('listing_images').insert(image_data).execute()
            
            print(f"      üì∏ {len(images)} images sauvegard√©es")
            return True
            
        except Exception as e:
            print(f"      ‚ùå Erreur sauvegarde images: {e}")
            return False
    
    def update_campaign_status(self, status: str, total_pages: int = 0, total_listings: int = 0):
        """Met √† jour le statut de la campagne"""
        if not self.supabase or not self.current_campaign_id:
            return False
        
        try:
            update_data = {
                'status': status,
                'total_pages': total_pages,
                'total_listings_found': total_listings
            }
            
            if status == 'completed':
                update_data['completed_at'] = datetime.now().isoformat()
            
            self.supabase.table('search_campaigns').update(update_data).eq('id', self.current_campaign_id).execute()
            print(f"‚úÖ Campagne mise √† jour: {status}")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur mise √† jour campagne: {e}")
            return False
    
    def load_cookies_from_file(self):
        """Charge les cookies depuis une variable d'environnement ou le fichier Netscape"""
        cookies = []

        # 1. Essayer de charger depuis la variable d'environnement
        cookies_env = os.getenv("LBC_COOKIES")
        if cookies_env:
            print("‚úÖ Cookies charg√©s depuis la variable d'environnement LBC_COOKIES")
            lines = cookies_env.splitlines()
        else:
            # 2. Sinon, charger depuis le fichier comme avant
            if not os.path.exists(self.cookies_file):
                print(f"‚ö†Ô∏è  Fichier cookies non trouv√©: {self.cookies_file}")
                return cookies
            try:
                with open(self.cookies_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
            except Exception as e:
                print(f"‚ùå Erreur lecture cookies: {e}")
                return []

        for line in lines:
            line = line.strip()
            # Ignorer les commentaires et lignes vides
            if line.startswith('#') or not line:
                continue
            # Format Netscape: domain\tflag\tpath\tsecure\texpiration\tname\tvalue
            parts = line.split('\t')
            if len(parts) >= 7:
                domain = parts[0]
                flag = parts[1] == 'TRUE'
                path = parts[2]
                secure = parts[3] == 'TRUE'
                expiration = int(parts[4]) if parts[4].isdigit() else None
                name = parts[5]
                value = parts[6]
                # Filtrer seulement les cookies LeBonCoin
                if 'leboncoin' in domain.lower():
                    cookie = {
                        'name': name,
                        'value': value,
                        'domain': domain,
                        'path': path,
                        'secure': secure,
                        'httpOnly': False
                    }
                    if expiration:
                        cookie['expiry'] = expiration
                    cookies.append(cookie)
        print(f"‚úÖ {len(cookies)} cookies LeBonCoin charg√©s")
        return cookies
    
    def setup_driver(self):
        """Configure le driver Chrome visible"""
        print("üöÄ Configuration du navigateur Chrome...")
        
        options = uc.ChromeOptions()
        
        # Options pour un navigateur plus naturel
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-blink-features=AutomationControlled")
        
        # Taille de fen√™tre r√©aliste
        options.add_argument("--window-size=1920,1080")
        options.add_argument("--start-maximized")
        
        # Mode headless optimis√© par d√©faut
        if self.headless:
            options.add_argument("--headless=new")  # Nouveau mode headless plus discret
            options.add_argument("--disable-gpu")
            options.add_argument("--disable-software-rasterizer")
            options.add_argument("--disable-background-timer-throttling")
            options.add_argument("--disable-backgrounding-occluded-windows")
            options.add_argument("--disable-renderer-backgrounding")
            options.add_argument("--disable-features=TranslateUI")
            options.add_argument("--disable-ipc-flooding-protection")
            options.add_argument("--disable-extensions")
            options.add_argument("--disable-plugins")
            options.add_argument("--disable-images")  # D√©sactiver le chargement des images pour plus de rapidit√©
            options.add_argument("--disable-javascript")  # D√©sactiver JS non essentiel
            # User agent r√©aliste
            options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            print("   Mode: Headless (invisible) - Configuration optimis√©e")
        else:
            print("   Mode: Visible")
        
        try:
            # Cr√©er le driver
            self.driver = uc.Chrome(options=options)
            
            # Scripts anti-d√©tection avanc√©s
            self.driver.execute_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
                
                // Masquer les traces d'automation
                Object.defineProperty(navigator, 'plugins', {
                    get: () => [1, 2, 3, 4, 5]
                });
                
                Object.defineProperty(navigator, 'languages', {
                    get: () => ['fr-FR', 'fr', 'en-US', 'en']
                });
                
                // Simuler une vraie r√©solution d'√©cran
                Object.defineProperty(screen, 'width', {
                    get: () => 1920
                });
                Object.defineProperty(screen, 'height', {
                    get: () => 1080
                });
            """)
            
            print("‚úÖ Navigateur configur√© et pr√™t")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur configuration navigateur: {e}")
            return False
    
    def load_cookies(self):
        """Charge les cookies dans le navigateur - Version rapide"""
        print("üç™ Chargement des cookies...")
        
        # D'abord aller sur LeBonCoin pour d√©finir le domaine
        try:
            self.driver.get("https://www.leboncoin.fr")
            time.sleep(1)  # R√©duit de 2s √† 1s
            
            # Charger les cookies depuis le fichier
            cookies = self.load_cookies_from_file()
            
            if not cookies:
                print("‚ö†Ô∏è  Aucun cookie LeBonCoin trouv√©, navigation normale")
                return False
            
            # Ajouter chaque cookie
            cookies_added = 0
            for cookie in cookies:
                try:
                    # Nettoyer le domaine si n√©cessaire
                    if cookie['domain'].startswith('.'):
                        cookie['domain'] = cookie['domain'][1:]
                    
                    self.driver.add_cookie(cookie)
                    cookies_added += 1
                    
                except Exception as e:
                    # Supprimer les logs d'erreur pour acc√©l√©rer
                    continue
            
            print(f"‚úÖ {cookies_added} cookies ajout√©s avec succ√®s")
            
            # Recharger la page pour appliquer les cookies
            self.driver.refresh()
            time.sleep(1)  # R√©duit de 3s √† 1s
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur chargement cookies: {e}")
            return False
    
    def human_like_scroll(self):
        """Scroll rapide mais naturel"""
        # Scroll plus rapide mais toujours naturel
        total_height = self.driver.execute_script("return document.body.scrollHeight")
        
        # Scroll en 2-3 mouvements rapides au lieu de nombreux petits
        scroll_positions = [total_height // 3, (total_height * 2) // 3, total_height]
        
        for position in scroll_positions:
            self.driver.execute_script(f"window.scrollTo(0, {position});")
            time.sleep(0.3)  # D√©lai r√©duit de 1.5s √† 0.3s
    
    def simulate_human_behavior(self):
        """Comportement humain minimal et rapide"""
        print("üé≠ Simulation comportement humain...")
        
        # Supprimer les mouvements de souris qui causent des erreurs
        # et ne sont pas n√©cessaires en headless
        
        # Scroll rapide uniquement
        self.human_like_scroll()
        
        # Pause minimale
        time.sleep(0.5)  # R√©duit de 1-3s √† 0.5s
    
    def navigate_to_homepage(self):
        """Navigue vers la page d'accueil avec cookies"""
        print("üè† 1. Navigation vers LeBonCoin avec cookies...")
        
        try:
            # Charger les cookies d'abord
            cookies_loaded = self.load_cookies()
            
            # V√©rifier le statut de la page
            current_url = self.driver.current_url
            print("‚úÖ Page d'accueil charg√©e")
            
            # Gestion des cookies si pr√©sente
            try:
                # Chercher le bouton d'acceptation des cookies
                cookie_selectors = [
                    '[data-qa-id="cookie-accept-all"]',
                    '[id*="cookie"]',
                    '[class*="cookie"]',
                    'button[aria-label*="Accept"]'
                ]
                
                for selector in cookie_selectors:
                    try:
                        cookie_btn = WebDriverWait(self.driver, 3).until(
                            EC.element_to_be_clickable((By.CSS_SELECTOR, selector))
                        )
                        cookie_btn.click()
                        print("‚úÖ Cookies accept√©s")
                        time.sleep(2)
                        break
                    except TimeoutException:
                        continue
                        
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Pas de popup cookies d√©tect√©")
            
            # Comportement humain
            self.simulate_human_behavior()
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur navigation page d'accueil: {e}")
            return False
    
    def search_saumur_houses(self):
        """Recherche les maisons √† S√®te - Version rapide"""
        print("üîç 2. Recherche maisons √† S√®te...")
        
        # Cr√©er une campagne Supabase si configur√©
        search_url = "https://www.leboncoin.fr/recherche?category=9&text=maison+S√®te"
        if self.supabase:
            search_config = {
                "city": "S√®te",
                "property_type": "maison",
                "category": 9,
                "transaction_type": "achat"
            }
            self.create_search_campaign(search_config, search_url)
        
        try:
            # Navigation directe plus rapide - pas de recherche dans la barre
            print("üöÄ Navigation directe vers les r√©sultats...")
            self.driver.get(search_url)
            
            # Attendre le chargement des r√©sultats - d√©lai r√©duit
            print("‚è≥ Attente du chargement des r√©sultats...")
            time.sleep(2)  # R√©duit de 5s √† 2s
            
            # V√©rifier qu'on est sur une page de r√©sultats
            current_url = self.driver.current_url
            if "recherche" in current_url:
                print("‚úÖ Page de r√©sultats charg√©e")
                return True
            else:
                print(f"‚ö†Ô∏è  URL inattendue: {current_url}")
                return False
                
        except Exception as e:
            print(f"‚ùå Erreur recherche: {e}")
            return False
    
    def extract_listings(self):
        """Extrait uniquement l'ID et l'URL des annonces - Version rapide"""
        print("üìä 3. Extraction des IDs et URLs...")
        
        # Scroll rapide pour charger le contenu
        print("   Scroll pour charger le contenu...")
        self.human_like_scroll()
        
        # D√©lai optimis√© en mode headless
        if self.headless:
            print("   ‚ö° Mode headless optimis√©...")
            time.sleep(1)  # R√©duit √† 1s pour mode invisible
        
        listings = []
        
        # S√©lecteurs optimis√©s - commencer par le plus probable
        selectors_to_try = [
            "[data-qa-id='aditem_container']",  # Le plus courant
            "[data-qa-id='aditem']", 
            "article[data-qa-id*='ad']",
            "div[data-qa-id*='ad']",
            ".ad-item",
            "[class*='aditem']"
        ]
        
        ad_containers = []
        
        # Essayer les s√©lecteurs avec timeout r√©duit
        for selector in selectors_to_try:
            try:
                print(f"   üîç Tentative: {selector}")
                WebDriverWait(self.driver, 8).until(  # R√©duit de 15s √† 8s
                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                )
                
                ad_containers = self.driver.find_elements(By.CSS_SELECTOR, selector)
                
                if ad_containers:
                    print(f"   ‚úÖ {len(ad_containers)} annonces trouv√©es")
                    break
                    
            except TimeoutException:
                continue
            except Exception:
                continue
        
        if not ad_containers:
            print("‚ùå Aucune annonce trouv√©e")
            return []
        
        try:            
            for i, container in enumerate(ad_containers):
                try:
                    listing = {}
                    
                    # Extraction rapide URL + ID
                    try:
                        link_elem = container.find_element(By.CSS_SELECTOR, "a")
                        listing['url'] = link_elem.get_attribute('href')
                        
                        # Extraire l'ID depuis l'URL
                        if listing['url']:
                            import re
                            id_match = re.search(r'/(\d+)(?:\?|$)', listing['url'])
                            listing['id'] = id_match.group(1) if id_match else ""
                        else:
                            listing['id'] = ""
                    except:
                        listing['url'] = ""
                        listing['id'] = ""
                    
                    # Ajouter si valide
                    if listing['url'] and listing['id']:
                        listings.append(listing)
                        if i < 5:  # Afficher seulement les 5 premiers pour acc√©l√©rer
                            print(f"      ‚úÖ ID: {listing['id']}")
                    
                    # Pas de d√©lai entre extractions pour acc√©l√©rer
                    
                except Exception:
                    continue
            
            print(f"üìã Total: {len(listings)} annonces extraites")
            self.results = listings
            
            # Ne plus envoyer vers Supabase ici - sera fait √† la fin du scraping complet
            # if self.supabase and listings:
            #     print("üîó Envoi vers Supabase...")
            #     self.send_listings_to_supabase(listings)
            
            return listings
            
        except Exception as e:
            print(f"‚ùå Erreur extraction: {e}")
            return []
    
    def save_results(self, format_type="both"):
        """Sauvegarde les r√©sultats (ID + URL uniquement)"""
        if not self.results:
            print("‚ùå Aucun r√©sultat √† sauvegarder")
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if format_type in ["csv", "both"]:
            filename = f"leboncoin_ids_urls_{timestamp}.csv"
            try:
                with open(filename, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=['id', 'url'])
                    writer.writeheader()
                    writer.writerows(self.results)
                print(f"üíæ CSV sauvegard√©: {filename}")
            except Exception as e:
                print(f"‚ùå Erreur CSV: {e}")
        
        if format_type in ["json", "both"]:
            filename = f"leboncoin_ids_urls_{timestamp}.json"
            try:
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(self.results, f, ensure_ascii=False, indent=2)
                print(f"üíæ JSON sauvegard√©: {filename}")
            except Exception as e:
                print(f"‚ùå Erreur JSON: {e}")
    
    def display_results(self):
        """Affiche les r√©sultats (ID + URL uniquement)"""
        if not self.results:
            print("‚ùå Aucun r√©sultat √† afficher")
            return
        
        print(f"\nüìä R√©sultats trouv√©s: {len(self.results)} annonces")
        print("=" * 60)
        
        for i, listing in enumerate(self.results[:10]):  # Afficher plus car moins d'infos
            print(f"{i+1}. ID: {listing['id']}")
            print(f"   üîó URL: {listing['url'][:80]}...")
            print()
        
        if len(self.results) > 10:
            print(f"... et {len(self.results) - 10} autres annonces")
    
    def find_pagination_selectors(self):
        """Trouve les s√©lecteurs de pagination"""
        print("üîç Recherche des s√©lecteurs de pagination...")
        
        pagination_selectors = [
            # S√©lecteurs pour le bouton "page suivante"
            'a[aria-label*="suivant"]',
            'a[aria-label*="next"]',
            'button[aria-label*="suivant"]',
            'button[aria-label*="next"]',
            # S√©lecteurs pour les num√©ros de page
            'a[data-spark-component="pagination-item"]',
            'button[data-spark-component="pagination-item"]',
            # S√©lecteurs g√©n√©riques
            '.pagination a',
            '[class*="pagination"] a',
            '[class*="pager"] a',
            # S√©lecteurs par num√©ro de page
            'a[href*="page=2"]',
            'a[href*="&p=2"]'
        ]
        
        found_selectors = []
        
        for selector in pagination_selectors:
            try:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    print(f"   ‚úÖ Trouv√© {len(elements)} √©l√©ments avec: {selector}")
                    for i, elem in enumerate(elements[:3]):  # Limiter √† 3 pour √©viter le spam
                        try:
                            text = elem.text.strip()
                            href = elem.get_attribute('href')
                            print(f"      [{i+1}] Texte: '{text}' | Href: {href[:60] if href else 'None'}...")
                        except:
                            pass
                    found_selectors.append((selector, elements))
            except Exception as e:
                continue
        
        return found_selectors
    
    def get_next_page_url(self):
        """Trouve l'URL de la page suivante"""
        try:
            # Chercher le bouton "page 2" ou "suivant"
            next_selectors = [
                'a[data-spark-component="pagination-item"][href*="page=2"]',
                'a[data-spark-component="pagination-item"][href*="&p=2"]',
                'a[aria-label*="Page 2"]',
                'a[href*="page=2"]',
                'a[href*="&p=2"]',
                'a:contains("2")',  # Lien contenant "2"
            ]
            
            for selector in next_selectors:
                try:
                    if ':contains(' in selector:
                        # Utiliser XPath pour :contains
                        xpath = f"//a[contains(text(), '2')]"
                        elements = self.driver.find_elements(By.XPATH, xpath)
                    else:
                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    
                    for elem in elements:
                        href = elem.get_attribute('href')
                        text = elem.text.strip()
                        
                        # V√©rifier que c'est bien un lien de pagination
                        if href and ('page=' in href or '&p=' in href) and text in ['2', 'Page 2', 'Suivant', 'Next']:
                            print(f"‚úÖ Page suivante trouv√©e: {text} -> {href}")
                            return href
                            
                except Exception as e:
                    continue
            
            print("‚ö†Ô∏è  Aucun lien vers la page suivante trouv√©")
            return None
            
        except Exception as e:
            print(f"‚ùå Erreur recherche page suivante: {e}")
            return None
    
    def detect_total_pages(self):
        """D√©tecte le nombre total de pages disponibles"""
        print("üî¢ D√©tection du nombre total de pages...")
        
        try:
            # Chercher les diff√©rents indicateurs du nombre de pages
            page_indicators = [
                # S√©lecteurs pour les num√©ros de page
                'a[data-spark-component="pagination-item"]',
                'button[data-spark-component="pagination-item"]',
                # S√©lecteurs g√©n√©riques
                '.pagination a',
                '[class*="pagination"] a',
                '[class*="pager"] a'
            ]
            
            max_page = 1
            all_page_numbers = set()
            
            for selector in page_indicators:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    
                    for elem in elements:
                        text = elem.text.strip()
                        href = elem.get_attribute('href')
                        
                        # Extraire le num√©ro de page du texte
                        if text.isdigit():
                            page_num = int(text)
                            all_page_numbers.add(page_num)
                            max_page = max(max_page, page_num)
                        
                        # Extraire le num√©ro de page de l'URL
                        if href:
                            import re
                            # Chercher page=X dans l'URL
                            page_match = re.search(r'page=(\d+)', href)
                            if page_match:
                                page_num = int(page_match.group(1))
                                all_page_numbers.add(page_num)
                                max_page = max(max_page, page_num)
                            
                            # Chercher &p=X dans l'URL
                            p_match = re.search(r'&p=(\d+)', href)
                            if p_match:
                                page_num = int(p_match.group(1))
                                all_page_numbers.add(page_num)
                                max_page = max(max_page, page_num)
                
                except Exception as e:
                    continue
            
            # Chercher aussi les indicateurs textuels comme "Page X sur Y"
            try:
                page_info_selectors = [
                    '[class*="pagination"] span',
                    '[class*="pager"] span',
                    'span[class*="page"]'
                ]
                
                for selector in page_info_selectors:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for elem in elements:
                        text = elem.text.strip()
                        # Chercher des patterns comme "Page 1 sur 10" ou "1 / 10"
                        import re
                        patterns = [
                            r'sur\s+(\d+)',  # "Page 1 sur 10"
                            r'/\s*(\d+)',    # "1 / 10"
                            r'de\s+(\d+)',   # "1 de 10"
                            r'of\s+(\d+)'    # "1 of 10"
                        ]
                        
                        for pattern in patterns:
                            match = re.search(pattern, text, re.IGNORECASE)
                            if match:
                                total_pages = int(match.group(1))
                                max_page = max(max_page, total_pages)
                                break
            except Exception as e:
                pass
            
            # Afficher les r√©sultats
            if all_page_numbers:
                sorted_pages = sorted(all_page_numbers)
                print(f"   üìÑ Pages d√©tect√©es: {sorted_pages}")
            
            print(f"‚úÖ Nombre total de pages d√©tect√©: {max_page}")
            
            # V√©rification de coh√©rence
            if max_page > 50:  # Limite de s√©curit√©
                print(f"‚ö†Ô∏è  Nombre de pages suspicieusement √©lev√© ({max_page}), limitation √† 10")
                max_page = 10
            
            return max_page
            
        except Exception as e:
            print(f"‚ùå Erreur d√©tection pages: {e}")
            return 1
    
    def scrape_multiple_pages(self, max_pages: int = 100, auto_detect: bool = True):
        """Scrape plusieurs pages de r√©sultats avec d√©tection dynamique"""
        
        all_listings = []
        current_page = 1
        detected_max_pages = max_pages
        
        # D√©tecter le nombre initial de pages
        if auto_detect:
            detected_max_pages = self.detect_total_pages()
            print(f"üìÑ D√©tection initiale: {detected_max_pages} pages")
        else:
            print(f"üìÑ Scraping manuel: {max_pages} pages maximum...")
        
        while current_page <= detected_max_pages:
            print(f"\nüìÑ === PAGE {current_page}/{detected_max_pages} ===")
            
            # Extraire les annonces de la page courante
            listings = self.extract_listings()
            
            if not listings:
                print(f"‚ùå Aucune annonce trouv√©e sur la page {current_page}")
                break
            
            print(f"‚úÖ {len(listings)} annonces extraites de la page {current_page}")
            all_listings.extend(listings)
            
            # Re-d√©tecter le nombre de pages √† chaque page (pagination dynamique)
            if auto_detect and current_page % 5 == 0:  # Re-v√©rifier toutes les 5 pages
                print("üîÑ Re-d√©tection du nombre de pages (pagination dynamique)...")
                new_detected_pages = self.detect_total_pages()
                
                if new_detected_pages > detected_max_pages:
                    print(f"üìà Nouvelles pages d√©tect√©es: {detected_max_pages} ‚Üí {new_detected_pages}")
                    detected_max_pages = new_detected_pages
                    
                    # Limite de s√©curit√© pour √©viter les boucles infinies
                    if detected_max_pages > 100:
                        print(f"‚ö†Ô∏è  Limite de s√©curit√© atteinte (100 pages), arr√™t du scraping")
                        detected_max_pages = 100
                        break
            
            # Si c'est la derni√®re page d√©tect√©e, v√©rifier s'il y en a d'autres
            if current_page >= detected_max_pages:
                if auto_detect:
                    print("üîç V√©rification finale des pages suppl√©mentaires...")
                    final_check = self.detect_total_pages()
                    if final_check > detected_max_pages:
                        print(f"üìà Pages suppl√©mentaires trouv√©es: {detected_max_pages} ‚Üí {final_check}")
                        detected_max_pages = final_check
                        
                        # Limite de s√©curit√©
                        if detected_max_pages > 100:
                            print(f"‚ö†Ô∏è  Limite de s√©curit√© atteinte (100 pages)")
                            break
                    else:
                        print("‚úÖ Aucune page suppl√©mentaire d√©tect√©e")
                        break
                else:
                    break
            
            # Chercher la page suivante
            next_page_num = current_page + 1
            next_url = self.get_next_page_url_by_number(next_page_num)
            
            if not next_url:
                print(f"‚ùå Impossible de trouver la page {next_page_num}")
                # Essayer de re-d√©tecter les pages au cas o√π
                if auto_detect:
                    print("üîÑ Re-d√©tection d'urgence...")
                    emergency_check = self.detect_total_pages()
                    if emergency_check > current_page:
                        print(f"üìà Pages trouv√©es lors de la re-d√©tection: {emergency_check}")
                        # Construire l'URL manuellement
                        current_url = self.driver.current_url
                        if 'page=' in current_url:
                            import re
                            next_url = re.sub(r'page=\d+', f'page={next_page_num}', current_url)
                        else:
                            separator = '&' if '?' in current_url else '?'
                            next_url = f"{current_url}{separator}page={next_page_num}"
                        print(f"üîß URL construite manuellement: {next_url}")
                    else:
                        break
                else:
                    break
            
            print(f"üîó Navigation vers la page {next_page_num}...")
            self.driver.get(next_url)
            
            # Attendre le chargement
            time.sleep(random.uniform(3, 5))
            
            # V√©rifier que la page s'est charg√©e
            try:
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "[data-qa-id='aditem_container']"))
                )
                print("‚úÖ Page suivante charg√©e")
            except TimeoutException:
                print("‚ùå Timeout lors du chargement de la page suivante")
                break
            
            current_page += 1
        
        print(f"\nüìä Total final: {len(all_listings)} annonces sur {current_page} page(s)")
        print(f"üìà Nombre final de pages d√©tect√©es: {detected_max_pages}")
        self.results = all_listings
        
        # Envoyer automatiquement toutes les donn√©es vers Supabase √† la fin
        if self.supabase and all_listings:
            print(f"\nüîó Envoi de toutes les donn√©es vers Supabase...")
            success = self.send_listings_to_supabase(all_listings)
            if success:
                print(f"‚úÖ {len(all_listings)} annonces envoy√©es vers Supabase avec succ√®s")
            else:
                print(f"‚ùå Erreur lors de l'envoi vers Supabase")
        elif self.supabase and not all_listings:
            print(f"‚ö†Ô∏è  Aucune donn√©e √† envoyer vers Supabase")
        
        return all_listings
    
    def get_next_page_url_by_number(self, page_num: int):
        """Trouve l'URL d'une page sp√©cifique par son num√©ro"""
        try:
            # Chercher le lien vers la page sp√©cifique
            selectors = [
                f'a[data-spark-component="pagination-item"][href*="page={page_num}"]',
                f'a[href*="page={page_num}"]',
                f'a[href*="&p={page_num}"]'
            ]
            
            for selector in selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for elem in elements:
                        href = elem.get_attribute('href')
                        text = elem.text.strip()
                        
                        if href and (f'page={page_num}' in href or f'&p={page_num}' in href):
                            print(f"‚úÖ Page {page_num} trouv√©e: {href}")
                            return href
                            
                except Exception as e:
                    continue
            
            # Si pas trouv√©, construire l'URL manuellement
            current_url = self.driver.current_url
            if 'page=' in current_url:
                # Remplacer le num√©ro de page existant
                import re
                new_url = re.sub(r'page=\d+', f'page={page_num}', current_url)
            else:
                # Ajouter le param√®tre page
                separator = '&' if '?' in current_url else '?'
                new_url = f"{current_url}{separator}page={page_num}"
            
            print(f"‚úÖ URL construite pour page {page_num}: {new_url}")
            return new_url
            
        except Exception as e:
            print(f"‚ùå Erreur recherche page {page_num}: {e}")
            return None
    
    def scrape_saumur_houses(self, max_pages: int = 100, auto_detect: bool = True):
        """Scrape les maisons √† S√®te avec pagination automatique"""
        print("üéØ Scraping Maisons S√®te - ID + URL uniquement")
        print("=" * 70)
        
        try:
            # 1. Navigation vers la page d'accueil
            if not self.navigate_to_homepage():
                return []
            
            # 2. Recherche
            if not self.search_saumur_houses():
                return []
            
            # 3. Scraping multi-pages
            return self.scrape_multiple_pages(max_pages, auto_detect)
            
        except Exception as e:
            print(f"‚ùå Erreur scraping: {e}")
            return []
    
    def close(self):
        """Ferme le navigateur"""
        if self.driver:
            if not self.headless:
                print("üîí Fermeture du navigateur dans 5 secondes...")
                time.sleep(5)  # Laisser le temps de voir les r√©sultats
            else:
                print("üîí Fermeture du navigateur invisible...")
                time.sleep(1)  # D√©lai r√©duit en mode headless
            try:
                self.driver.quit()
                print("‚úÖ Navigateur ferm√©")
            except:
                pass

def main():
    """Fonction principale"""
    parser = argparse.ArgumentParser(description="Scraper LeBonCoin avec Navigateur Visible et Cookies + Supabase")
    parser.add_argument("--visible", action="store_true", help="Mode visible (par d√©faut: invisible)")
    parser.add_argument("--headless", action="store_true", help="Mode invisible (par d√©faut)")
    parser.add_argument("--format", choices=["csv", "json", "both"], default="both", help="Format de sortie")
    parser.add_argument("--keep-open", action="store_true", help="Garder le navigateur ouvert")
    parser.add_argument("--cookies", default="cookies.txt", help="Fichier de cookies √† utiliser")
    parser.add_argument("--pages", type=int, default=100, help="Nombre maximum de pages √† scraper (d√©faut: 100)")
    parser.add_argument("--auto-detect", action="store_true", default=True, help="D√©tecter automatiquement le nombre de pages (d√©faut: activ√©)")
    parser.add_argument("--no-auto-detect", action="store_false", dest="auto_detect", help="D√©sactiver la d√©tection automatique")
    
    # Param√®tres Supabase
    parser.add_argument("--supabase-url", help="URL Supabase")
    parser.add_argument("--supabase-key", help="Cl√© API Supabase")
    
    args = parser.parse_args()
    
    if not SELENIUM_AVAILABLE:
        print("‚ùå Selenium requis. Installez avec:")
        print("   pip install undetected-chromedriver selenium")
        return
    
    print(f"üî• Scraper LeBonCoin + Supabase")
    
    # Cr√©er le scraper pour v√©rifier la configuration Supabase
    # Mode invisible par d√©faut, sauf si --visible est sp√©cifi√©e
    headless_mode = not args.visible if hasattr(args, 'visible') and args.visible else True
    
    scraper = LeBonCoinScraperVisible(
        headless=headless_mode, 
        cookies_file=args.cookies,
        supabase_url=args.supabase_url,
        supabase_key=args.supabase_key
    )
    
    # Afficher le statut Supabase correct
    if scraper.supabase:
        print(f"   Supabase: ‚úÖ Configur√© et connect√©")
    else:
        print(f"   Supabase: ‚ùå Non configur√©")
    
    try:
        # Lancer le scraping
        results = scraper.scrape_saumur_houses(max_pages=args.pages, auto_detect=args.auto_detect)
        
        # Mettre √† jour le statut de la campagne
        if scraper.supabase and scraper.current_campaign_id:
            total_scraped = len(results) if results else 0
            scraper.update_campaign_status('completed', total_listings=total_scraped)
        
        if results:
            # Afficher les r√©sultats
            scraper.display_results()
            
            # Sauvegarder localement aussi
            scraper.save_results(args.format)
            
            pages_text = "toutes les pages d√©tect√©es" if args.auto_detect else f"{args.pages} page(s) maximum"
            print(f"\nüéâ Scraping termin√© avec succ√®s!")
            print(f"üìä {len(results)} annonces extraites sur {pages_text}")
            
            if scraper.supabase:
                print(f"üíæ Donn√©es sauvegard√©es dans Supabase (Campagne: {scraper.current_campaign_id})")
            
            if args.keep_open:
                input("\n‚è∏Ô∏è  Appuyez sur Entr√©e pour fermer le navigateur...")
        else:
            print("\n‚ùå Aucune annonce extraite")
    
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Arr√™t demand√© par l'utilisateur")
        if scraper.supabase and scraper.current_campaign_id:
            scraper.update_campaign_status('failed')
    except Exception as e:
        print(f"\n‚ùå Erreur inattendue: {e}")
        if scraper.supabase and scraper.current_campaign_id:
            scraper.update_campaign_status('failed')
    finally:
        scraper.close()

if __name__ == "__main__":
    main() 